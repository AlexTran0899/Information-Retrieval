/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 138, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 95, in tokenize_and_save
    doc = read_file(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 51, in read_file
    return nlp(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1037, in __call__
    doc = self._ensure_doc(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1128, in _ensure_doc
    return self.make_doc(doc_like)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1117, in make_doc
    raise ValueError(
ValueError: [E088] Text of length 1361831 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 138, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 95, in tokenize_and_save
    doc = read_file(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 51, in read_file
    return nlp(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1037, in __call__
    doc = self._ensure_doc(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1128, in _ensure_doc
    return self.make_doc(doc_like)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1117, in make_doc
    raise ValueError(
ValueError: [E088] Text of length 1361831 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 142, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 95, in tokenize_and_save
    doc = read_file(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 51, in read_file
    return nlp(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1037, in __call__
    doc = self._ensure_doc(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1128, in _ensure_doc
    return self.make_doc(doc_like)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1117, in make_doc
    raise ValueError(
ValueError: [E088] Text of length 1361831 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 159, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 94, in tokenize_and_save
    doc = read_file(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 51, in read_file
    return nlp(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1037, in __call__
    doc = self._ensure_doc(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1128, in _ensure_doc
    return self.make_doc(doc_like)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1117, in make_doc
    raise ValueError(
ValueError: [E088] Text of length 1361831 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 145, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 94, in tokenize_and_save
    doc = read_file(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 51, in read_file
    return nlp(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1037, in __call__
    doc = self._ensure_doc(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1128, in _ensure_doc
    return self.make_doc(doc_like)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1117, in make_doc
    raise ValueError(
ValueError: [E088] Text of length 1361831 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 142, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 96, in tokenize_and_save
    doc = read_file(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 52, in read_file
    return nlp(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1049, in __call__
    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]
  File "spacy/pipeline/trainable_pipe.pyx", line 52, in spacy.pipeline.trainable_pipe.TrainablePipe.__call__
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/pipeline/tok2vec.py", line 126, in predict
    tokvecs = self.model.predict(docs)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/model.py", line 334, in predict
    return self._func(self, X, is_train=False)[0]
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/layers/chain.py", line 54, in forward
    Y, inc_layer_grad = layer(X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/model.py", line 310, in __call__
    return self._func(self, X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/layers/with_array.py", line 42, in forward
    return cast(Tuple[SeqT, Callable], _list_forward(model, Xseq, is_train))
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/layers/with_array.py", line 77, in _list_forward
    Yf, get_dXf = layer(Xf, is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/model.py", line 310, in __call__
    return self._func(self, X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/layers/chain.py", line 54, in forward
    Y, inc_layer_grad = layer(X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/model.py", line 310, in __call__
    return self._func(self, X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/layers/residual.py", line 41, in forward
    Y, backprop_layer = model.layers[0](X, is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/model.py", line 310, in __call__
    return self._func(self, X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/layers/chain.py", line 54, in forward
    Y, inc_layer_grad = layer(X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/model.py", line 310, in __call__
    return self._func(self, X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/layers/chain.py", line 54, in forward
    Y, inc_layer_grad = layer(X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/model.py", line 310, in __call__
    return self._func(self, X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/layers/chain.py", line 54, in forward
    Y, inc_layer_grad = layer(X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/model.py", line 310, in __call__
    return self._func(self, X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/layers/layernorm.py", line 24, in forward
    N, mu, var = _get_moments(model.ops, X)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/layers/layernorm.py", line 75, in _get_moments
    var: Floats2d = X.var(axis=1, keepdims=True) + 1e-08
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/numpy/core/_methods.py", line 176, in _var
    x = um.multiply(x, x, out=x)
KeyboardInterrupt
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 142, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 109, in tokenize_and_save
    alt = get_html_attribute_value(temp, 'alt="')
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 69, in get_html_attribute_value
    elif(text[p1] != tag[p2]):
KeyboardInterrupt
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 142, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 97, in tokenize_and_save
    matcher(doc)
  File "spacy/matcher/matcher.pyx", line 334, in spacy.matcher.matcher.Matcher.__call__
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 24, in match_default
    if not token._.matched:
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/tokens/underscore.py", line 64, in __getattr__
    key = self._get_key(name)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/tokens/underscore.py", line 93, in _get_key
    return ("._.", name, self._start, self._end)
KeyboardInterrupt
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 142, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 96, in tokenize_and_save
    doc = read_file(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 52, in read_file
    return nlp(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1037, in __call__
    doc = self._ensure_doc(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1128, in _ensure_doc
    return self.make_doc(doc_like)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1117, in make_doc
    raise ValueError(
ValueError: [E088] Text of length 1361831 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 142, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 96, in tokenize_and_save
    doc = read_file(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 52, in read_file
    return nlp(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1037, in __call__
    doc = self._ensure_doc(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1128, in _ensure_doc
    return self.make_doc(doc_like)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1117, in make_doc
    raise ValueError(
ValueError: [E088] Text of length 1361831 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 149, in <module>
    start_time = time.time()
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 96, in tokenize_and_save
    
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 52, in read_file
    def read_file(file_name):
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1049, in __call__
    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]
  File "spacy/pipeline/trainable_pipe.pyx", line 52, in spacy.pipeline.trainable_pipe.TrainablePipe.__call__
  File "spacy/pipeline/transition_parser.pyx", line 264, in spacy.pipeline.transition_parser.Parser.predict
  File "spacy/pipeline/transition_parser.pyx", line 285, in spacy.pipeline.transition_parser.Parser.greedy_parse
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/model.py", line 334, in predict
    return self._func(self, X, is_train=False)[0]
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/ml/tb_framework.py", line 34, in forward
    step_model = ParserStepModel(
  File "spacy/ml/parser_model.pyx", line 250, in spacy.ml.parser_model.ParserStepModel.__init__
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/model.py", line 310, in __call__
    return self._func(self, X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/layers/chain.py", line 54, in forward
    Y, inc_layer_grad = layer(X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/model.py", line 310, in __call__
    return self._func(self, X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/layers/chain.py", line 54, in forward
    Y, inc_layer_grad = layer(X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/model.py", line 310, in __call__
    return self._func(self, X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/layers/chain.py", line 54, in forward
    Y, inc_layer_grad = layer(X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/model.py", line 310, in __call__
    return self._func(self, X, is_train=is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/layers/with_array.py", line 36, in forward
    return cast(Tuple[SeqT, Callable], _ragged_forward(model, Xseq, is_train))
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/layers/with_array.py", line 91, in _ragged_forward
    Y, get_dX = layer(Xr.dataXd, is_train)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/thinc/model.py", line 310, in __call__
    return self._func(self, X, is_train=is_train)
KeyboardInterrupt
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
  warnings.warn(Warnings.W108)
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 153, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 100, in tokenize_and_save
    doc = read_file(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 56, in read_file
    return nlp(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1049, in __call__
    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/pipeline/attributeruler.py", line 142, in __call__
    matches = self.match(doc)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/pipeline/attributeruler.py", line 149, in match
    matches = self.matcher(doc, allow_missing=True, as_spans=False)
KeyboardInterrupt

/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 138, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 95, in tokenize_and_save
    doc = read_file(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 51, in read_file
    return nlp(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1037, in __call__
    doc = self._ensure_doc(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1128, in _ensure_doc
    return self.make_doc(doc_like)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1117, in make_doc
    raise ValueError(
ValueError: [E088] Text of length 1361831 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 138, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 95, in tokenize_and_save
    doc = read_file(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 51, in read_file
    return nlp(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1037, in __call__
    doc = self._ensure_doc(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1128, in _ensure_doc
    return self.make_doc(doc_like)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1117, in make_doc
    raise ValueError(
ValueError: [E088] Text of length 1361831 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 142, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 95, in tokenize_and_save
    doc = read_file(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 51, in read_file
    return nlp(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1037, in __call__
    doc = self._ensure_doc(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1128, in _ensure_doc
    return self.make_doc(doc_like)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1117, in make_doc
    raise ValueError(
ValueError: [E088] Text of length 1361831 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 159, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 94, in tokenize_and_save
    doc = read_file(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 51, in read_file
    return nlp(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1037, in __call__
    doc = self._ensure_doc(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1128, in _ensure_doc
    return self.make_doc(doc_like)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1117, in make_doc
    raise ValueError(
ValueError: [E088] Text of length 1361831 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Traceback (most recent call last):
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 145, in <module>
    tokenize_and_save(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 94, in tokenize_and_save
    doc = read_file(file_name)
  File "/Users/a1234/Desktop/Information Retrieval/hw1-part2/tokenizer.py", line 51, in read_file
    return nlp(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1037, in __call__
    doc = self._ensure_doc(text)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1128, in _ensure_doc
    return self.make_doc(doc_like)
  File "/Users/a1234/Library/Python/3.9/lib/python/site-packages/spacy/language.py", line 1117, in make_doc
    raise ValueError(
ValueError: [E088] Text of length 1361831 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
/Users/a1234/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
